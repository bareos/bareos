#!/bin/bash
set -e
set -o pipefail
set -u
#
# This systemtest tests the plugin functionality
# of the Bareos FD by using the supplied module
#   bareos-fd-local-fileset.py
#
# The module will backup some files.
# This plugin is not intended for production,
# but is only a minimal example that shows
# how to use the python plugin interface.
# File attributes like uses and times will not be saved.
#
TestName="$(basename "$(pwd)")"
export TestName
bucket_name=bareos-test

JobName=backup-bareos-fd
#shellcheck source=../environment.in
. ./environment

JobName=backup-bareos-fd
#shellcheck source=../scripts/functions
. "${rscripts}"/functions
"${rscripts}"/cleanup
"${rscripts}"/setup

# shortcut for s3cmd
S3="${S3CMD} --no-check-certificate --config ${S3CFG}"



# Fill ${BackupDirectory} with data.
setup_data

# create files to test the temporary-file and the stream-download path
prefetch_size=$(( $(grep prefetch_size etc/libcloud_config.ini | cut -d '=' -f 2) ))

#backup via temp file
dd if=/dev/random \
  of="${tmp}/data/object-size-downloads-to-temporary-file" \
  bs=$(( prefetch_size -1 )) count=1

#backup via stream object using the plugin process itself
dd if=/dev/random \
  of="${tmp}/data/object-size-downloads-with-plugin-process" \
  bs=$(( prefetch_size +1 )) count=1

"${rscripts}"/start_minio.sh "$MINIO_PORT" "$TestName"

# create s3 content for test
${S3} rb --recursive --force s3://$bucket_name || echo "s3://$bucket_name does not exist"
${S3} mb s3://$bucket_name


# this test does not work with links and some other weird files as they would already
# have a changed name by syncing to S3 using s3cmd
find ${tmp}/data/weird-files -type l -exec rm {} \;
find ${tmp}/data/weird-files -links +1 -type f -exec rm {} \;
rm ${tmp}/data/weird-files/fifo*
rm ${tmp}/data/weird-files/newline*
rm ${tmp}/data/weird-files/tab*
# the following file also makes problems
rm ${tmp}/data/weird-files/filename-with-non-utf8-bytestring*
# s3cmd does not sync empty dirs
rmdir ${tmp}/data/weird-files/big-X
rmdir ${tmp}/data/weird-files/subdir

${S3} sync "$BackupDirectory" s3://$bucket_name

start_test

cat <<END_OF_DATA >$tmp/bconcmds
@$out /dev/null
messages
@$out $tmp/log1.out
setdebug level=100 storage=File
setdebug level=100 client=bareos-fd trace=1 timestamp=1
label volume=TestVolume001 storage=File pool=Full
run job=$JobName yes
status director
status client
status storage=File
wait
messages
@#
@# now do a restore
@#
@$out $tmp/log2.out
wait
restore client=bareos-fd fileset=PluginTest where=$tmp/bareos-restores select all done
yes
wait
messages
quit
END_OF_DATA

run_bareos "$@"
check_for_zombie_jobs storage=File
stop_bareos

check_two_logs
list=( $(find "${BackupDirectory}" -type f) )
# Using check_restore_only_files_diff instead of check_restore_diff
# to don'"t diff the file attributes, because they are not saved
#check_restore_only_files_diff "${list[@]}"

if ! diff -r tmp/data tmp/bareos-restores/$bucket_name/data; then
  export estat=1
fi

"${rscripts}"/stop_minio.sh "$TestName"

end_test
